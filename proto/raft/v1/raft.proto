syntax = "proto3";

package raft.v1;

message VoteRequest {
  // current term of candidate
  int64 term = 1;
  // id of candidate
  string candidate = 2;
  // index of candidate's last log entry
  int64 last_log_index = 3;
  // term of candidate's last log entry
  int64 last_log_term = 4;
}

message VoteResponse {
  // current term, for candidate to update itself
  int64 term = 1;
  // true means candidate received vote
  bool vote_granted = 2;
}

enum LogType {
  kUserLog = 0;
  kRaftCmd = 1;
}

message LogEntry {
  int64 index = 2;
  int64 term = 3;
  bytes log_data = 4;
  LogType type = 5;
}

message AppendEntriesRequest {
  // leader's term
  int64 term = 2;
  // so followers can redirect clients
  string leader = 3;
  // index of log entry immediately precedding new ones
  int64 prev_log_index = 4;
  // term of prevLogIndex entry
  int64 prev_log_term = 5;
  // log entries to store, empty for heartbeats
  LogEntry entries = 6;
  // leader's commitIndex
  int64 leader_commit = 7;
}

message AppendEntriesResponse {
  // currentTerm, for leader to update itself
  int64 term = 2;
  // true if follower contained entry matching prevLogIndex and prevLogTerm
  bool success = 3;
}
/*
  Raft Protocol Implementation:
    - Leader Election
    - Log Replication
    - Safty
    - Log Compaction
    - Membership Change
  Raft is an algorithm for managing Replicated Log.
  firstly, it will elect a Leader from all servers, the Leader will take
  responsibilities to manage Replicated Log, it receives requests from Client,
  and copy those logs to Follwers, and notify Follwers to commit those logs,

  One Leader is can reduce the complexity for managing Replicated Logs, by
  define the Leader, The problem is split to 3 sub problems
    - Leader Election: When cluster bootstrap, or existing Leader down, we
  should be able to elect a new one
    - Log ReplicationL When Leader receives requests from clients, we should be
  able to copy those requests to Followes to ensure the consistency
    - Safety: IF ANY OF SERVER APPLY ENTRY, THE OTHER SERVERS ARE IMPOSSIBLE TO
  APPLY A DIFFERENT ENTRY IN THE SAME INDEX

  Properties:
    - Election Safety
    - Leader Append-Only
    - Log Matching
    - Leader Completeness
    - State Machine Safety

  State:
    Persistent States on all servers:
    - currentTerm
    - votedFor
    - log[]

    Volatile States on all servers:
    - commitIndex
    - lastApplied

    Volatitle State on Leaders:
    - nextIndex[]
    - matchIndex[]

  Term is a arbitrary time span, use integer to identify them,
  in every term, the cluster start from `Leader Election`, if no leader elected,
  just continue Raft ensures in every term, there is only one Leader, term is
  similar to logic clock

  Basic consistency can be reached with these two kinds of RPCs
*/
service Raft {
  // Implementation:
  // 1. Reply false if term < currentTerm
  // 2. If votedFor is null or candidateId, and candidate's log is
  // at least as up-to-date as receiver's log, grant vote
  //
  // broadcastTime << electionTimeout << MTBF
  rpc Vote(VoteRequest) returns (VoteResponse) {}

  // Implementation:
  // 1. Reply false if term < currentTerm
  // 2. Reply false if log doesn't contain an entry at prevLogIndex whose term
  // matches prevLogTerm
  // 3. If an existing entry conflicts with a new one, delete the exsting entry
  // and all that follow it
  // 4. append any new entries not already in the log
  // 5. if leaderCommit > commitIndex, set commitIndex = min(leaderCommit, index
  // of last new entry)

  // AppendEntries with empty logs is regarded as Heartbeat
  rpc AppendEntries(AppendEntriesRequest) returns (AppendEntriesResponse) {}

  // Clustr Membership Changes
  // RESTRICTION: THE CLUSTER CAN ONLY ADD OR REMOVE ONE MACHINE AT THE SAME
  // TIME

  // rpc AddServer(AddServerRequest) returns (AddServerResponse) {}
  // rpc RemoveServer(RemoveServerRequest) returns (RemoveServerRequest) {}
}